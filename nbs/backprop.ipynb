{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "x = -2; y = 5; z = -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nodes along our graph. f is the output\n",
    "q = x + y\n",
    "f = q * z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backpropogating through\n",
    "df_dz = q # derivative of f wrt z is q (power rule)\n",
    "df_dq = z\n",
    "\n",
    "# what is df_dx? it is the partial of q wrt x times the partial of f wrt q\n",
    "# this is because we want to know how much q changes due to x and because\n",
    "# q influences f, we want to know how much f changes because of q. The\n",
    "# product here is the multivariate chain rule.\n",
    "df_dx = 1.00 * df_dq # dq_dx = 1.00 and we already know df_dq. multiply along the graph of partials\n",
    "df_dy = 1.00 * df_dq # dq_dy = 1.00\n",
    "\n",
    "# Storing the partial df_dq lets us cache this variable and efficiently reuse it later\n",
    "# Note: we now know how much our output, f, changes wrt each input. In the cases of \n",
    "# x and y, our output is independent of changes to the input. The input z will drive\n",
    "# changes to the output. The xy components of the gradient are directly proportional\n",
    "# to the input value of z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-4.0, -4.0, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[df_dx, df_dy, df_dz] # this is the gradient\n",
    "# If we now wish to perform optimization through stochastic gradient descent we could\n",
    "# update weights on our inputs to move them in the opposite direction of the gradient\n",
    "# (minimization) in order to minimize our output value. In this toy example the output\n",
    "# is not meaningful, but in a real neural network the output is the output of our loss\n",
    "# function, and minimizing this has great meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = [2, -3, -3] # random weights and inputs\n",
    "x = [-1, -2]\n",
    "\n",
    "# forward pass (dot product followed by activation function)\n",
    "dot = w[0]*x[0] + w[1]*x[1] + w[2]\n",
    "f = 1.0 / (1 + math.exp(-dot)) # sigmoid\n",
    "\n",
    "# backward pass\n",
    "ddot = (1 - f) * f # this may look bizarre but algebraically it is the derivation of the gradient of the sigmoid\n",
    "dx = [w[0] * ddot, w[1] * ddot]\n",
    "dw = [x[0] * ddot, x[1] * ddot, 1 * ddot]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Function\n",
    "\n",
    "$f(x, y) = \\frac{x + \\sigma(y)}{\\sigma(x) + (x+y)^{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
